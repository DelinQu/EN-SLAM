<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta property="og:type" content="website" />
  <meta content="summary_large_image" name="twitter:card" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

  <title>Implicit Event-RGBD Neural SLAM</title>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9Z7HCWJNBC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-9Z7HCWJNBC');
  </script>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preload" as="style"
    href="https://fonts.googleapis.com/css2?family=Asap:wght@700&family=Source+Sans+3:wght@400;700&display=swap">
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css2?family=Asap:wght@700&family=Source+Sans+3:wght@400;700&display=swap">
  <link href="style.css" rel="stylesheet" type="text/css" />
</head>

<body>
  <div class="section">
    <div class="container">
      <div class="title-row">
        <h1 class="title">Implicit Event-RGBD Neural SLAM<h1>
      </div>
      <div class="row">
        <div class="author-col">
          <a href="https://delinqu.github.io/" target="_blank" class="author-text">
            Delin Qu<sup>1,2*</sup>
          </a>
        </div>
        <div class="author-col">
          <a href="https://github.com/NPU-YanChi" target="_blank" class="author-text">
            Chi Yan<sup>2,5*</sup>
          </a>
        </div>
        <div class="author-col">
          <a href="#" target="_blank" class="author-text">
            Dong Wang<sup>2</sup>
            <span class="superscript"></span>
          </a>
        </div>
        <div class="author-col">
          <a href="#" target="_blank" class="author-text">
            Jie Ying<sup>3</sup>
            <span class="superscript"></span>
          </a>
        </div>
      </div>

      <div class="row">
        <div class="author-col">
          <a href="#" target="_blank" class="author-text">
            Qizhi Chen<sup>2</sup>
          </a>
        </div>
        <div class="author-col">
          <a href="#" target="_blank" class="author-text">
            Yiting Zhang<sup>2</sup>
          </a>
        </div>
        <div class="author-col">
          <a href="https://www.danxurgb.net" target="_blank" class="author-text">
            Dan Xu<sup>5</sup>
          </a>
        </div>
        <div class="author-col">
          <a href="#" target="_blank" class="author-text">
            Bin Zhao<sup>2,4†</sup>
          </a>
        </div>
        <div class="author-col">
          <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=zh-CN" target="_blank" class="author-text">
            Xuelong Li<sup>2†</sup>
          </a>
        </div>
      </div>
    </div>
    <p id="uc-berkeley"><sup>1</sup>Fudan University, &nbsp; <sup>2</sup>Shanghai AI Laboratory, &nbsp; <sup>3</sup>Shanghai Jiao Tong University</h1>
    <p id="uc-berkeley"><sup>4</sup>Northwestern Polytechnical University, &nbsp; &nbsp; <sup>5</sup>Hong Kong University of Sciences and Technology</h1>
    <div class="row button-row">
      <a class="link-button" href="https://arxiv.org/abs/2311.11013" target="_blank" class="link-block">Paper</a>
      <a class="link-button" href="https://github.com/DelinQu/EN-SLAM" class="link-block">Code</a>
      <a class="link-button" href="#" class="link-block">Dataset</a>
      <a class="link-button" href="#" class="link-block">YouTube</a>
    </div>
    <p class="tldr">
      <b>TL;DR: EN-SLAM</b>, the first event-RGBD implicit neural SLAM that leverages event stream and RGBD to overcome challenges in motion blur and lighting variation scenes; differentiable CRF rendering &rarr; shared radiance field &rarr; RGB+event data; consecutive difference constraints of the event stream &rarr; temporal aggregating.
    </p>
    <!-- <video id="main-video" muted autoplay controls playsinline loop>
      <source id="mp4" src="https://www.youtube.com/embed/Or-yvKHUrZ0" type="video/mp4">
    </video> -->
    <!-- <iframe id="main-video" height="404.9" src="https://www.youtube.com/embed/Or-yvKHUrZ0" title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen></iframe> -->

    <div id="content">
      <h2 class="section-header">Overview</h2>
      <div class="paragraph">
        <p>
          Implicit neural SLAM has achieved remarkable progress recently. Nevertheless, existing methods face significant challenges in non-ideal scenarios, such as motion blur or lighting variation, which often leads to issues like convergence failures, localization drifts, and distorted mapping. To address these challenges, we propose EN-SLAM, the first event-RGBD implicit neural SLAM framework, which effectively leverages the high rate and high dynamic range advantages of event data for tracking and mapping. Specifically, EN-SLAM proposes a differentiable CRF (Camera Response Function) rendering technique to generate distinct RGB and event camera data via a shared radiance field, which is optimized by learning a unified implicit representation with the captured event and RGBD supervision. Moreover, based on the temporal difference property of events, we propose a temporal aggregating optimization strategy for the event joint tracking and global bundle adjustment, capitalizing on the consecutive difference constraints of events, significantly enhancing tracking accuracy and robustness. Finally, we construct the simulated dataset DEV-Indoors and real captured dataset DEV-Reals containing 6 scenes, 17 sequences with practical motion blur and lighting changes for evaluations. Experimental results show that our method outperforms the SOTA methods in both tracking ATE and mapping ACC with a real-time 17 FPS in various challenging environments.
        </p>
        <p><b>Contributions:</b></p>
        <ul>
          <li> We present <b>EN-SLAM</b>, the first event-RGBD implicit neural SLAM framework that efficiently leverages event stream and RGBD to overcome challenges in extreme motion blur and lighting variation scenes.
          <li> A differentiable CRF rendering technique is proposed to map a unified representation in the shared radiance field to RGB and event camera data for addressing the significant distinction between event and RGB. A temporal aggregating optimization strategy that capitalizes the consecutive difference constraints of the event stream is present and significantly improves the camera tracking accuracy and robustness.
          <li>  We construct a simulated <b>DEV-Indoors</b> and real captured <b>DEV-Reals</b> dataset containing 17 sequences with practical motion blur and lighting changes. A wide range of evaluations demonstrate competitive real-time performance under various challenging environments.
        </ul>
      </div>

      <!-- <h2 class="section-header">Motivation</h2>
      <div class="img-container">
        <div class="paragraph">
          Existing methods suffer from two main drawbacks. Firstly, they face challenges in estimating the accurate
          correction field due to the uniform velocity assumption, leading to significant image correction errors under
          complex motion. Secondly, the drastic occlusion in dynamic scenes prevents current solutions from achieving
          better image quality because of the inherent difficulties in aligning and aggregating multiple frames
        </div>
        <img class="wide-img" src="static/images/motivation.png" alt="motivation">
        <div class="paragraph">
          To tackle these challenges, we model the curvilinear trajectory of pixels analytically and propose a
          geometry-based Quadratic Rolling Shutter (QRS) motion solver, which precisely estimates the high-order
          correction field of individual pixels. Besides, to reconstruct high-quality occlusion frames in dynamic
          scenes, we present a 3D video architecture that effectively Aligns and Aggregates multi-frame context.
        </div>
      </div>

      <h2 class="section-header">Overview</h2>
      <div class="img-container">
        <div class="paragraph">
          Overview of the proposed method. We aim to estimate precise correction fields in nonlinear motion with the QRS
          motion solver and synthesize high-quality frames against dynamic scenes with extreme occlusion by a
          self-alignment 3D video architecture RSA2-Net.
        </div>
        <img class="wide-img" src="static/images/pipeline.png" alt="motivation">
        <div class="paragraph">
          The object moves at variable velocity and has a complex curvilinear trajectory. Note that the object has only
          2 Dof in the image plane, and the time interval between frames is very small. Thus, we use a quadratic motion
          model to formalize the curvilinear trajectory of the pixel.
        </div>
        <img class="wide-img" src="static/images/solver.png" alt="motivation">
      </div>

      <h2 class="section-header">Quantitative Analysis and Generalization Ability</h2>
      <div class="img-container">
        <div class="paragraph">
          The results reported in table show that the proposed method outperforms the other eight RSC methods by large
          margins on Carla-RS, Fascte-RS, BS-RSC and ACC datasets. These superior performances significantly demonstrate
          the effectiveness of our model on highly dynamic scenes with occlusion and real-world curvilinear movements.
          We performed cross-tests on three datasets the proposed method demonstrates strong generalization performance,
          benefiting from the QRS motion solver.
        </div>
        <img class="wide-img" src="static/images/quantitative.png" alt="motivation">
      </div>

      <h2 class="section-header">Visual Comparisons</h2>
      <div class="img-container">
        <div class="paragraph">
          Visual comparisons on Carla-RS, Fastec-RS, BS-RSC and ACC datasets. The proposed method can effectively
          correct the rolling shutter distortion in complex nonlinear and dynamic scenes with extreme occlusion.
        </div>
        <img class="wide-img" src="static/images/visual1.png" alt="motivation">
        <img class="wide-img" src="static/images/visual2.png" alt="motivation">
        <div class="paragraph">
        </div>
      </div> -->

      <h2 class="section-header">Citation</h2>
      <div class="citation">
        <pre id="codecell0">
@inproceedings{
    anonymous2024implicit,
    title={Implicit Event-{RGBD} Neural {SLAM}},
    author={Anonymous},
    booktitle={Conference on Computer Vision and Pattern Recognition 2024},
    year={2024},
    url={https://openreview.net/forum?id=OjQP10Pgnp}
  }</pre>
      </div>
      <br><br>
      <div class="paragraph-center">For more information, check out the paper, code, and YouTube video:</div>
      <div class="row button-row">
        <a class="link-button" href="https://arxiv.org/abs/2303.18125" target="_blank" class="link-block">Paper</a>
        <a class="link-button" href="https://github.com/DelinQu/qrsc" class="link-block">Code</a>
        <a class="link-button" href="https://www.youtube.com/watch?v=Or-yvKHUrZ0" class="link-block">YouTube</a>
      </div>
    </div>

  </div>
  </div>
</body>

</html>